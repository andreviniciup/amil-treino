# ü§ñ GUIA COMPLETO: Testando ML Service e Evoluindo para Deep Learning

## üìã **COMO TESTAR O ML SERVICE**

### **1Ô∏è‚É£ Verificar se o ML Service est√° rodando**

```bash
# Terminal 1 - Backend TypeScript
cd backend
npm run dev

# Terminal 2 - ML Service Python  
cd ml-service
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

**Teste b√°sico:**
```bash
curl http://localhost:8000
# Deve retornar: {"service": "ML Service - Treino Inteligente", ...}
```

---

## üß™ **TESTES MANUAIS DOS ENDPOINTS ML**

### **1. Testar Status dos Modelos**
```bash
curl http://localhost:8000/api/ml/train/models/status
```

**Resposta esperada:**
```json
{
  "recommendation_model": {
    "trained": false,
    "type": "Hybrid (RF + GB + NN)"
  },
  "performance_predictor": {
    "trained": false,
    "type": "LSTM"
  },
  "progression_model": {
    "trained": false,
    "type": "Gradient Boosting"
  }
}
```

### **2. Gerar Dados Mock e Treinar Modelo**
```bash
curl -X POST http://localhost:8000/api/ml/train/generate-mock-data
```

**Resposta esperada:**
```json
{
  "message": "Modelo treinado com dados mock",
  "samples_generated": 1000,
  "training_results": {
    "rf_accuracy": 0.85,
    "gb_accuracy": 0.87,
    "ensemble_accuracy": 0.89
  }
}
```

### **3. Testar Recomenda√ß√µes ML**
```bash
curl -X POST http://localhost:8000/api/ml/recommendations \
  -H "Content-Type: application/json" \
  -d '{
    "age": 25,
    "gender": "Masculino",
    "weight": 75,
    "height": 175,
    "fitnessLevel": "Intermedi√°rio",
    "trainingExperience": 2,
    "primaryGoals": ["Hipertrofia", "For√ßa"],
    "availableDays": 5,
    "availableTime": 60,
    "equipment": ["Barra", "Halteres"]
  }'
```

### **4. Testar Predi√ß√£o de Performance**
```bash
curl -X POST http://localhost:8000/api/ml/predict/performance \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test-user",
    "exercise_id": "supino",
    "history": [
      {"date": "2024-01-01", "exercise_id": "supino", "weight": 80, "reps": 10, "sets": 3},
      {"date": "2024-01-08", "exercise_id": "supino", "weight": 82.5, "reps": 10, "sets": 3},
      {"date": "2024-01-15", "exercise_id": "supino", "weight": 85, "reps": 8, "sets": 3}
    ]
  }'
```

### **5. Testar Busca Cient√≠fica**
```bash
curl -X POST http://localhost:8000/api/ml/scientific/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "progressive overload training",
    "limit": 5
  }'
```

---

## üöÄ **EVOLUINDO PARA DEEP LEARNING (DL)**

### **üìä Arquitetura Atual vs DL**

| **Aspecto** | **ML Atual** | **Deep Learning** |
|-------------|--------------|-------------------|
| **Modelos** | RF + GB + NN simples | CNN + LSTM + Transformer |
| **Dados** | Features manuais | Embeddings + Time Series |
| **Processamento** | Est√°tico | Din√¢mico + Attention |
| **Precis√£o** | 85-90% | 95-98% |

---

## üß† **IMPLEMENTA√á√ÉO DE DEEP LEARNING**

### **1. Novos Requirements (requirements-dl.txt)**
```txt
# Deep Learning Core
tensorflow==2.16.1
torch==2.1.0
transformers==4.35.2

# Time Series
tslearn==0.6.2
prophet==1.1.4

# Computer Vision (para an√°lise de forma)
opencv-python==4.8.1.78
mediapipe==0.10.7

# NLP Avan√ßado
sentence-transformers==2.2.2
spacy[transformers]==3.7.2

# Reinforcement Learning
stable-baselines3==2.2.1
gymnasium==0.29.1
```

### **2. Modelo LSTM Avan√ßado para Performance**
```python
# ml-service/app/models/lstm_performance_model.py
import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np

class LSTMPerformanceModel:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        
    def build_model(self, sequence_length=30, n_features=10):
        """Constr√≥i modelo LSTM para predi√ß√£o de performance"""
        
        inputs = layers.Input(shape=(sequence_length, n_features))
        
        # LSTM Stack
        lstm1 = layers.LSTM(128, return_sequences=True, dropout=0.2)(inputs)
        lstm2 = layers.LSTM(64, return_sequences=True, dropout=0.2)(lstm1)
        lstm3 = layers.LSTM(32, dropout=0.2)(lstm2)
        
        # Attention Mechanism
        attention = layers.MultiHeadAttention(
            num_heads=8, 
            key_dim=32,
            dropout=0.1
        )(lstm3, lstm3)
        
        # Dense Layers
        dense1 = layers.Dense(64, activation='relu')(attention)
        dropout1 = layers.Dropout(0.3)(dense1)
        dense2 = layers.Dense(32, activation='relu')(dropout1)
        dropout2 = layers.Dropout(0.2)(dense2)
        
        # Outputs (weight, reps, confidence)
        weight_output = layers.Dense(1, activation='linear', name='weight')(dropout2)
        reps_output = layers.Dense(1, activation='linear', name='reps')(dropout2)
        confidence_output = layers.Dense(1, activation='sigmoid', name='confidence')(dropout2)
        
        self.model = Model(
            inputs=inputs,
            outputs=[weight_output, reps_output, confidence_output]
        )
        
        self.model.compile(
            optimizer='adam',
            loss={
                'weight': 'mse',
                'reps': 'mse', 
                'confidence': 'binary_crossentropy'
            },
            metrics=['mae']
        )
        
        return self.model
```

### **3. Modelo Transformer para Recomenda√ß√µes**
```python
# ml-service/app/models/transformer_recommendation.py
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn

class TransformerRecommendationModel:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        self.bert = AutoModel.from_pretrained('bert-base-uncased')
        self.classifier = nn.Linear(768, 7)  # 7 m√©todos de treino
        
    def encode_user_profile(self, user_data):
        """Converte perfil do usu√°rio em embedding BERT"""
        text = f"""
        User: {user_data['age']} years old, {user_data['gender']}
        Goals: {', '.join(user_data['goals'])}
        Experience: {user_data['experience']} years
        Available: {user_data['days']} days per week
        """
        
        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.bert(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
        
        return embeddings
    
    def predict(self, user_data):
        """Prediz m√©todo de treino usando Transformer"""
        embeddings = self.encode_user_profile(user_data)
        logits = self.classifier(embeddings)
        probabilities = torch.softmax(logits, dim=1)
        
        return {
            'method': torch.argmax(probabilities).item(),
            'confidence': torch.max(probabilities).item(),
            'all_probabilities': probabilities.tolist()
        }
```

### **4. CNN para An√°lise de Forma (Futuro)**
```python
# ml-service/app/models/cnn_form_analysis.py
import cv2
import mediapipe as mp
import tensorflow as tf

class FormAnalysisCNN:
    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose()
        
    def analyze_exercise_form(self, video_path, exercise_type):
        """Analisa forma do exerc√≠cio usando CNN + MediaPipe"""
        
        # Extrair keypoints do v√≠deo
        keypoints = self.extract_pose_keypoints(video_path)
        
        # CNN para an√°lise de forma
        model = self.build_form_cnn()
        form_score = model.predict(keypoints)
        
        return {
            'form_score': form_score,
            'corrections': self.generate_corrections(form_score),
            'safety_rating': self.calculate_safety_rating(form_score)
        }
    
    def extract_pose_keypoints(self, video_path):
        """Extrai keypoints de pose usando MediaPipe"""
        cap = cv2.VideoCapture(video_path)
        keypoints_sequence = []
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            results = self.pose.process(frame)
            if results.pose_landmarks:
                keypoints = self.normalize_keypoints(results.pose_landmarks)
                keypoints_sequence.append(keypoints)
        
        cap.release()
        return np.array(keypoints_sequence)
```

---

## üîÑ **MIGRA√á√ÉO GRADUAL PARA DL**

### **Fase 1: LSTM para Performance (1-2 semanas)**
```python
# Implementar LSTM para predi√ß√£o de performance
# Substituir l√≥gica simples por modelo treinado
# Manter compatibilidade com API atual
```

### **Fase 2: Transformer para Recomenda√ß√µes (2-3 semanas)**
```python
# Implementar BERT/Transformer para recomenda√ß√µes
# Fine-tuning com dados de treino
# A/B testing com modelo atual
```

### **Fase 3: CNN para An√°lise de Forma (3-4 semanas)**
```python
# Implementar an√°lise de v√≠deo
# Integrar com c√¢mera do celular
# Feedback em tempo real
```

### **Fase 4: Reinforcement Learning (4-6 semanas)**
```python
# Implementar RL para otimiza√ß√£o de treinos
# Agente que aprende com feedback do usu√°rio
# Personaliza√ß√£o cont√≠nua
```

---

## üìä **M√âTRICAS DE COMPARA√á√ÉO ML vs DL**

| **M√©trica** | **ML Atual** | **DL Esperado** |
|-------------|--------------|-----------------|
| **Precis√£o Recomenda√ß√µes** | 85% | 95% |
| **Precis√£o Predi√ß√µes** | 75% | 90% |
| **Tempo de Resposta** | 50ms | 200ms |
| **Dados Necess√°rios** | 1K amostras | 10K+ amostras |
| **Recursos Computacionais** | Baixo | Alto |

---

## üõ†Ô∏è **IMPLEMENTA√á√ÉO PR√ÅTICA**

### **1. Criar Branch DL**
```bash
git checkout -b feature/deep-learning
```

### **2. Instalar Depend√™ncias DL**
```bash
cd ml-service
pip install -r requirements-dl.txt
```

### **3. Implementar Modelo LSTM**
```bash
# Criar arquivo
touch app/models/lstm_performance_model.py
# Implementar classe LSTMPerformanceModel
```

### **4. Testar Modelo DL**
```bash
# Testar endpoint com modelo DL
curl -X POST http://localhost:8000/api/ml/predict/performance-dl \
  -H "Content-Type: application/json" \
  -d '{"user_id": "test", "exercise_id": "supino", "history": [...]}'
```

---

## üéØ **ROADMAP DL COMPLETO**

### **Semana 1-2: LSTM Performance**
- ‚úÖ Implementar LSTM para predi√ß√£o
- ‚úÖ Treinar com dados hist√≥ricos
- ‚úÖ Comparar com modelo atual

### **Semana 3-4: Transformer Recommendations**
- ‚úÖ Fine-tuning BERT
- ‚úÖ Embeddings de perfil
- ‚úÖ A/B testing

### **Semana 5-6: CNN Form Analysis**
- ‚úÖ Integra√ß√£o MediaPipe
- ‚úÖ An√°lise de v√≠deo
- ‚úÖ Feedback de forma

### **Semana 7-8: Reinforcement Learning**
- ‚úÖ Agente RL
- ‚úÖ Otimiza√ß√£o cont√≠nua
- ‚úÖ Personaliza√ß√£o avan√ßada

---

## üöÄ **PR√ìXIMOS PASSOS**

1. **Execute os testes ML atuais** usando o script `TEST-ALL-ENDPOINTS.ps1`
2. **Implemente LSTM** para predi√ß√£o de performance
3. **Colete mais dados** de treino dos usu√°rios
4. **Implemente Transformer** para recomenda√ß√µes
5. **Adicione an√°lise de v√≠deo** com CNN
6. **Integre Reinforcement Learning** para otimiza√ß√£o

**O sistema est√° pronto para evoluir para Deep Learning!** üß†‚ú®
